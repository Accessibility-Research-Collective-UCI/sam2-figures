{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1049f59-91d4-4444-a443-b30f3df128a9",
   "metadata": {},
   "source": [
    "# Finetuning SAM 2 for Scientific Figures\n",
    "This code is adapated from the following:\n",
    "- [https://www.datacamp.com/tutorial/sam2-fine-tuning](https://www.datacamp.com/tutorial/sam2-fine-tuning)\n",
    "- [https://learnopencv.com/finetuning-sam2/](https://learnopencv.com/finetuning-sam2/)\n",
    "- [https://medium.com/data-science/train-fine-tune-segment-anything-2-sam-2-in-60-lines-of-code-928dd29a63b3](https://medium.com/data-science/train-fine-tune-segment-anything-2-sam-2-in-60-lines-of-code-928dd29a63b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a746a28-7050-412a-9e8f-7fa80292bc19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855070b-b454-4688-87e9-f2f844e95781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds():\n",
    "    SEED_VALUE = 42\n",
    "    random.seed(SEED_VALUE)\n",
    "    np.random.seed(SEED_VALUE)\n",
    "    torch.manual_seed(SEED_VALUE)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED_VALUE)\n",
    "        torch.cuda.manual_seed_all(SEED_VALUE)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    " \n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2879c7-99be-460a-8e87-f55b0dc8e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsons(input_dir):\n",
    "    json_files = [\n",
    "        pos_json for pos_json in os.listdir(input_dir) if pos_json.endswith(\".json\")\n",
    "    ]\n",
    "    output = [{}] * len(json_files)\n",
    "\n",
    "    # get the image and bounding box\n",
    "    for index, file in enumerate(json_files):  # [0:1]:\n",
    "        with open(os.path.join(input_dir, file), \"r\") as input_file:\n",
    "            data = json.load(input_file)\n",
    "            output[index] = {\n",
    "                \"document\": data[\"shapes\"][0][\"image_name\"],\n",
    "                \"component_name\": data[\"name\"],\n",
    "                \"image\": data[\"origin_image\"],\n",
    "                \"bounding_box\": data[\"shapes\"][0][\"points\"],\n",
    "            }\n",
    "\n",
    "            # DEBUGGING\n",
    "            # if data[\"origin_image\"] == \"./dataset/image/W19-6501-Figure4-1.png\":\n",
    "            #     print(index)\n",
    "            #     print(output[index])\n",
    "            #     print(file)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def plot_image_with_bounding_box(data):\n",
    "    image_path = data[\"image\"]\n",
    "    bbox_coords = data[\"bounding_box\"][0]  # Assuming one polygon\n",
    "    component_name = data[\"component_name\"]\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "\n",
    "    # Load the image\n",
    "    img = mpimg.imread(image_path)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot original image\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis(\"on\")\n",
    "\n",
    "    # Plot image with bounding box\n",
    "    axs[1].imshow(img)\n",
    "    axs[1].set_title(\"Image with Bounding Box\")\n",
    "\n",
    "    # Create a polygon patch\n",
    "    polygon = patches.Polygon(\n",
    "        bbox_coords, closed=True, edgecolor=\"red\", linewidth=2, facecolor=\"none\"\n",
    "    )\n",
    "    axs[1].add_patch(polygon)\n",
    "\n",
    "    # Optionally annotate the component name at the top-left of the bounding box\n",
    "    x, y = bbox_coords[0]\n",
    "    axs[1].text(\n",
    "        x,\n",
    "        y - 10,\n",
    "        component_name,\n",
    "        color=\"red\",\n",
    "        fontsize=12,\n",
    "        path_effects=[PathEffects.withStroke(linewidth=2, foreground=\"white\")],\n",
    "    )\n",
    "\n",
    "    axs[1].axis(\"on\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def convert_box_to_mask(data):\n",
    "    img = cv2.imread(data[\"image\"])\n",
    "    box = np.array(data[\"bounding_box\"][0])\n",
    "    mask = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, pts=[box], color=(255, 0, 0))\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ddd2b-e665-499e-8dbf-72b6f156e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./dataset/\"\n",
    "train_metadata = os.path.join(data_dir, \"train\")\n",
    "val_metadata = os.path.join(data_dir, \"val\")\n",
    "images_dir = os.path.join(data_dir, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21507c90-9919-4efd-81d5-890e17145ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: verify training data is completely separate from valiation and testing data\n",
    "train_data = read_jsons(train_metadata)\n",
    "val_data = read_jsons(val_metadata)\n",
    "\n",
    "print(f\"Number training datapoints: {len(train_data)}\")\n",
    "print(f\"Number validation datapoints: {len(val_data)}\")\n",
    "\n",
    "print(f\"Number unique figures for training: {len(set([x['image'] for x in train_data]))}\")\n",
    "print(f\"Number unique figures for validation: {len(set([x['image'] for x in val_data]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd28e6a2-3f99-425a-8e25-d6b339d260af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0147f534-ced8-4279-908f-147d706369fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8504d01-a087-4f1e-abdb-bdabdf2bcc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_with_bounding_box(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a2b89-5e65-48b6-9003-28ed8c24b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch(data, visualize_data=False):\n",
    "    ent = data[np.random.randint(len(data))]\n",
    "    Img = cv2.imread(ent[\"image\"])[...,::-1]\n",
    "    ann_map = convert_box_to_mask(ent)\n",
    "\n",
    "    if Img is None or ann_map is None:\n",
    "        print(\n",
    "            f\"Error: Could not read image or mask from path {ent['image']} or {ent['annotation']}\"\n",
    "        )\n",
    "        return None, None, None, 0\n",
    "\n",
    "    r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]])\n",
    "    Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n",
    "    ann_map = cv2.resize(\n",
    "        ann_map,\n",
    "        (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)),\n",
    "        interpolation=cv2.INTER_NEAREST,\n",
    "    )\n",
    "\n",
    "    binary_mask = np.zeros_like(ann_map, dtype=np.uint8)\n",
    "    points = []\n",
    "    inds = np.unique(ann_map)[1:]\n",
    "    for ind in inds:\n",
    "        mask = (ann_map == ind).astype(np.uint8)\n",
    "        binary_mask = np.maximum(binary_mask, mask)\n",
    "\n",
    "    eroded_mask = cv2.erode(binary_mask, np.ones((5, 5), np.uint8), iterations=1)\n",
    "    coords = np.argwhere(eroded_mask > 0)\n",
    "    if len(coords) > 0:\n",
    "        for _ in inds:\n",
    "            yx = np.array(coords[np.random.randint(len(coords))])\n",
    "            points.append([yx[1], yx[0]])\n",
    "    points = np.array(points)\n",
    "\n",
    "    if visualize_data:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.imshow(Img)\n",
    "        plt.axis(\"on\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Binarized Mask\")\n",
    "        plt.imshow(binary_mask, cmap=\"gray\")\n",
    "        plt.axis(\"on\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Binarized Mask with Points\")\n",
    "        plt.imshow(binary_mask, cmap=\"gray\")\n",
    "        colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "        for i, point in enumerate(points):\n",
    "            plt.scatter(point[0], point[1], c=colors[i % len(colors)], s=100)\n",
    "        plt.axis(\"on\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    binary_mask = np.expand_dims(binary_mask, axis=-1)\n",
    "    binary_mask = binary_mask.transpose((2, 0, 1))\n",
    "    points = np.expand_dims(points, axis=1)\n",
    "    return Img, binary_mask, points, len(inds), ent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5fb93d-d641-4502-a0ec-60c81a571254",
   "metadata": {},
   "outputs": [],
   "source": [
    "Img1, masks1, points1, num_masks, image_info = read_batch(train_data, visualize_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42124899-2f71-41d8-83c4-a598413b5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(predictor, train_data, step, mean_iou):\n",
    "    with torch.amp.autocast(device_type=\"cuda\"):\n",
    "        image, mask, input_point, num_masks, image_info = read_batch(\n",
    "            train_data, visualize_data=False\n",
    "        )\n",
    "\n",
    "        if image is None or mask is None or num_masks == 0:\n",
    "            print(\"Image or Mask was None\")\n",
    "            return None, None, None, image_info\n",
    "\n",
    "        input_label = np.ones((num_masks, 1))\n",
    "\n",
    "        if not isinstance(input_point, np.ndarray) or not isinstance(\n",
    "            input_label, np.ndarray\n",
    "        ):\n",
    "            print(\"Input label not the right type\")\n",
    "            return None, None, None, image_info\n",
    "\n",
    "        if input_point.size == 0 or input_label.size == 0:\n",
    "            print(\"Input not a coordinate\")\n",
    "            return None, None, None, image_info\n",
    "\n",
    "        predictor.set_image(image)\n",
    "        mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(\n",
    "            input_point, input_label, box=None, mask_logits=None, normalize_coords=True\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            unnorm_coords is None\n",
    "            or labels is None\n",
    "            or unnorm_coords.shape[0] == 0\n",
    "            or labels.shape[0] == 0\n",
    "        ):\n",
    "            print(\"Prompt could not be prepared\")\n",
    "            return None, None, None, image_info\n",
    "\n",
    "        sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n",
    "            points=(unnorm_coords, labels), boxes=None, masks=None\n",
    "        )\n",
    "\n",
    "        batched_mode = unnorm_coords.shape[0] > 1\n",
    "        high_res_features = [\n",
    "            feat_level[-1].unsqueeze(0)\n",
    "            for feat_level in predictor._features[\"high_res_feats\"]\n",
    "        ]\n",
    "\n",
    "        low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "            image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "            image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=True,\n",
    "            repeat_image=batched_mode,\n",
    "            high_res_features=high_res_features,\n",
    "        )\n",
    "\n",
    "        prd_masks = predictor._transforms.postprocess_masks(\n",
    "            low_res_masks, predictor._orig_hw[-1]\n",
    "        )\n",
    "\n",
    "        gt_mask = torch.tensor(mask.astype(np.float32)).cuda()\n",
    "        prd_mask = torch.sigmoid(prd_masks[:, 0])\n",
    "\n",
    "        seg_loss = (\n",
    "            -gt_mask * torch.log(prd_mask + 1e-6)\n",
    "            - (1 - gt_mask) * torch.log((1 - prd_mask) + 1e-6)\n",
    "        ).mean()\n",
    "\n",
    "        inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "        iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "\n",
    "        score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "        loss = seg_loss + score_loss * 0.05\n",
    "\n",
    "        loss = loss / accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(predictor.model.parameters(), max_norm=1.0)\n",
    "\n",
    "        if step % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            predictor.model.zero_grad()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # save model\n",
    "        if step % 1000 == 0:\n",
    "            FINE_TUNED_MODEL = FINE_TUNED_MODEL_NAME + \"_\" + str(step) + \".pt\"\n",
    "            torch.save(predictor.model.state_dict(), FINE_TUNED_MODEL)\n",
    "\n",
    "        try:\n",
    "            mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\n",
    "        except Exception as e:\n",
    "          print(f\"An unexpected error occurred: {e}\")\n",
    "          print(\n",
    "            f\"Loss was None -- Image: {train_data['document']} Metadata: {train_data['document']}\"\n",
    "          )\n",
    "\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    return mean_iou, current_lr, seg_loss, image_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca00064-0bc3-4b5f-bfba-4f6e658c3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FigureDataset(Dataset):\n",
    "    def __init__(self, list_of_items):\n",
    "        self.items = list_of_items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]\n",
    "\n",
    "def read_batch_validate(batch_of_dicts):\n",
    "    \"\"\"\n",
    "    batch_of_dicts: list of dicts, length = batch_size\n",
    "    Returns:\n",
    "      images:    np.ndarray [B, H_max, W_max, 3]\n",
    "      masks:     np.ndarray [B, 1, H_max, W_max]\n",
    "      pts_list:  list of length B, each an [Ni,1,2] array\n",
    "      n_masks:   list of length B\n",
    "      infos:     list of original dicts\n",
    "    \"\"\"\n",
    "    imgs, msks, pts_list, n_masks, infos = [], [], [], [], []\n",
    "    heights, widths = [], []\n",
    "\n",
    "    # 1) load & preprocess each sample, record its size\n",
    "    for ent in batch_of_dicts:\n",
    "        Img = cv2.imread(ent[\"image\"])[..., ::-1]\n",
    "        ann_map = convert_box_to_mask(ent)\n",
    "        if Img is None or ann_map is None:\n",
    "            print(f\"Error reading {ent['image']}\")\n",
    "            continue\n",
    "\n",
    "        # resize to fit within 1024×1024\n",
    "        r = min(1024/Img.shape[1], 1024/Img.shape[0])\n",
    "        new_w, new_h = int(Img.shape[1]*r), int(Img.shape[0]*r)\n",
    "        Img = cv2.resize(Img, (new_w, new_h))\n",
    "        ann_map = cv2.resize(ann_map, (new_w, new_h),\n",
    "                             interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # build binary mask\n",
    "        binary_mask = np.zeros_like(ann_map, np.uint8)\n",
    "        labels = np.unique(ann_map); labels = labels[labels != 0]\n",
    "        for lbl in labels:\n",
    "            binary_mask = np.maximum(binary_mask,\n",
    "                                     (ann_map == lbl).astype(np.uint8))\n",
    "\n",
    "        # pick one random point per region\n",
    "        eroded = cv2.erode(binary_mask, np.ones((5,5),np.uint8), 1)\n",
    "        coords = np.argwhere(eroded > 0)\n",
    "        pts = []\n",
    "        for _ in labels:\n",
    "            if len(coords):\n",
    "                y, x = coords[np.random.randint(len(coords))]\n",
    "                pts.append([x, y])\n",
    "        pts = np.array(pts, np.int32)[:, None, :]  # [Ni,1,2]\n",
    "\n",
    "        imgs.append(Img)\n",
    "        msks.append(binary_mask[None, ...])  # [1,H,W]\n",
    "        pts_list.append(pts)\n",
    "        n_masks.append(len(labels))\n",
    "        infos.append(ent)\n",
    "        heights.append(Img.shape[0])\n",
    "        widths.append(Img.shape[1])\n",
    "\n",
    "    # 2) pad all to common size\n",
    "    H_max, W_max = max(heights), max(widths)\n",
    "    padded_imgs, padded_msks = [], []\n",
    "    for img, m in zip(imgs, msks):\n",
    "        h, w = img.shape[:2]\n",
    "        pad_img = np.zeros((H_max, W_max, 3), dtype=img.dtype)\n",
    "        pad_img[:h, :w] = img\n",
    "        padded_imgs.append(pad_img)\n",
    "\n",
    "        pad_m = np.zeros((1, H_max, W_max), dtype=m.dtype)\n",
    "        pad_m[:, :h, :w] = m\n",
    "        padded_msks.append(pad_m)\n",
    "\n",
    "    images = np.stack(padded_imgs, axis=0)   # [B,H_max,W_max,3]\n",
    "    masks  = np.stack(padded_msks, axis=0)   # [B,1,H_max,W_max]\n",
    "    return images, masks, pts_list, n_masks, infos\n",
    "\n",
    "def validate(\n",
    "    predictor,\n",
    "    optimizer,\n",
    "    test_data,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    val_loader = DataLoader(\n",
    "        FigureDataset(test_data),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=read_batch_validate\n",
    "    )\n",
    "\n",
    "    predictor.model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_iou  = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    with torch.no_grad(), torch.amp.autocast(device_type=device):\n",
    "        for images_np, masks_np, pts_list, n_masks, infos in val_loader:\n",
    "            # Keep images as NumPy for set_image(); convert masks to torch\n",
    "            gt_masks = torch.from_numpy(masks_np).to(device, dtype=torch.float32)\n",
    "            B = gt_masks.size(0)\n",
    "            batch_losses, batch_ious = [], []\n",
    "\n",
    "            for i in range(B):\n",
    "                # feed the original H×W×3 NumPy image to predictor\n",
    "                predictor.set_image(images_np[i])\n",
    "                gt_mask = gt_masks[i, 0]  # [H,W]\n",
    "\n",
    "                pts = pts_list[i]\n",
    "                labels = np.ones((n_masks[i], 1), dtype=np.int32)\n",
    "\n",
    "                # prepare prompts\n",
    "                mask_input, unnorm_coords, lbls, _ = predictor._prep_prompts(\n",
    "                    pts, labels, box=None, mask_logits=None, normalize_coords=True\n",
    "                )\n",
    "                if unnorm_coords is None or lbls is None:\n",
    "                    continue\n",
    "\n",
    "                # encode & decode\n",
    "                sp_emb, dn_emb = predictor.model.sam_prompt_encoder(\n",
    "                    points=(unnorm_coords, lbls), boxes=None, masks=None\n",
    "                )\n",
    "                high_res = [\n",
    "                    lvl[-1].unsqueeze(0)\n",
    "                    for lvl in predictor._features[\"high_res_feats\"]\n",
    "                ]\n",
    "                low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "                    image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "                    image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "                    sparse_prompt_embeddings=sp_emb,\n",
    "                    dense_prompt_embeddings=dn_emb,\n",
    "                    multimask_output=True,\n",
    "                    repeat_image=False,\n",
    "                    high_res_features=high_res,\n",
    "                )\n",
    "\n",
    "                # upsample to original size\n",
    "                prd_masks = predictor._transforms.postprocess_masks(\n",
    "                    low_res_masks, predictor._orig_hw[-1]\n",
    "                )\n",
    "                prd_mask = torch.sigmoid(prd_masks[:, 0])  # [H,W]\n",
    "\n",
    "                # segmentation loss\n",
    "                seg_loss = (\n",
    "                    -gt_mask * torch.log(prd_mask + 1e-6)\n",
    "                    - (1 - gt_mask) * torch.log(1 - prd_mask + 1e-6)\n",
    "                ).mean()\n",
    "\n",
    "                # IoU & score loss\n",
    "                inter = (gt_mask * (prd_mask > 0.5)).sum((-1, -2))\n",
    "                union = (\n",
    "                    gt_mask.sum((-1, -2))\n",
    "                    + (prd_mask > 0.5).sum((-1, -2))\n",
    "                    - inter\n",
    "                )\n",
    "                iou = inter / (union + 1e-6)\n",
    "                score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "\n",
    "                loss = seg_loss + 0.05 * score_loss\n",
    "                batch_losses.append(loss.item())\n",
    "                batch_ious.append(iou.mean().item())\n",
    "\n",
    "            if batch_losses:\n",
    "                total_loss += sum(batch_losses) / len(batch_losses)\n",
    "                total_iou  += sum(batch_ious)   / len(batch_ious)\n",
    "                total_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_batches)\n",
    "    avg_iou  = total_iou  / max(1, total_batches)\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    return {\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"avg_iou\":  avg_iou,\n",
    "        \"lr\":       current_lr,\n",
    "        \"batches\":  total_batches,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdc8ab-e1d2-47c7-9056-ca81f63131aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM Model and prepare for training\n",
    "sam2_checkpoint = \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"sam2.1_hiera_l.yaml\"\n",
    " \n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "# predictor.model.image_encoder.train(True)\n",
    "predictor.model.sam_mask_decoder.train(True)\n",
    "predictor.model.sam_prompt_encoder.train(True)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "NO_OF_STEPS = 40000\n",
    "FINE_TUNED_MODEL_NAME = \"./finetuned-models/fine_tuned_sam2\"\n",
    " \n",
    "optimizer = torch.optim.AdamW(params=predictor.model.parameters(),\n",
    "                              lr=0.00005,\n",
    "                              weight_decay=0.001)\n",
    " \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.6)\n",
    "accumulation_steps = 16\n",
    "\n",
    "# store training output\n",
    "train_output = [{}] * NO_OF_STEPS\n",
    "train_mean_iou = 0\n",
    "validation_output = []\n",
    "\n",
    "output_str = \"\"\n",
    "for step in tqdm(range(1, NO_OF_STEPS + 1)):\n",
    "    new_train_mean_iou, train_lr, train_seg_loss, train_image_info = train(predictor, train_data, step, train_mean_iou)\n",
    "\n",
    "    if new_train_mean_iou is not None:\n",
    "        train_mean_iou = new_train_mean_iou\n",
    "\n",
    "    # track iou and loss\n",
    "    if all(v is not None for v in [train_lr, train_seg_loss, validation_output]):\n",
    "        # update output\n",
    "        train_output[step - 1] = {\n",
    "            \"mean_iou\": float(train_mean_iou),\n",
    "            \"train_lr\": train_lr,\n",
    "            \"seg_loss\": float(train_seg_loss.item()),\n",
    "        }\n",
    "\n",
    "        # print status\n",
    "        output_str = f\"Train: {train_image_info['document']} / {train_image_info['image']}\"\n",
    "        if step % 500 == 0:\n",
    "            print(f\"Training: Step {step} | Current LR = {train_lr:.6f} | Avg IoU = {train_mean_iou:.6f} | Avg Seg Loss = {train_seg_loss:.6f}\")\n",
    "\n",
    "    # run validation every 1000 steps\n",
    "    if step % 500 == 0:\n",
    "        curr_validation_output = validate(predictor, optimizer, val_data, batch_size=len(val_data))\n",
    "        validation_output.append(curr_validation_output)\n",
    "        print(f\"Validation: Avg IoU = {curr_validation_output['avg_iou']:.6f}, Avg Loss = {curr_validation_output['avg_loss']:.6f}\")\n",
    "        \n",
    "    print(\n",
    "        \"{:<300}\".format(output_str), end='\\r'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e24a02-a7f7-46ef-b249-50e110535cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./finetuned-models/training-log_sam2.1_hiera_large_{NO_OF_STEPS}-epochs_2000-step_06-gamma_{accumulation_steps}-acc.json', 'w') as f:\n",
    "    json.dump(train_output, f)\n",
    "\n",
    "with open(f'./finetuned-models/validation-log_sam2.1_hiera_large_{NO_OF_STEPS}-epochs_2000-step_06-gamma_{accumulation_steps}-acc.json', 'w') as f:\n",
    "    json.dump(validation_output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943344a7-e771-4ce5-b076-e9ab63a04689",
   "metadata": {},
   "source": [
    "### Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbf880-a5be-4798-8c7b-1b93a1329035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values\n",
    "train_loss = [entry['seg_loss'] for entry in train_output if len(entry) > 0][::100]\n",
    "train_iou = [entry['mean_iou'] for entry in train_output if len(entry) > 0][::100]\n",
    "\n",
    "val_loss = [entry['avg_loss'] for entry in validation_output]\n",
    "val_iou = [entry['avg_iou'] for entry in validation_output]\n",
    "\n",
    "# Plotting\n",
    "epochs_train = [x * 100 for x in list(range(1, len(train_loss) + 1))]\n",
    "epochs_val = [x * 500 for x in list(range(1, len(val_loss)+1))]\n",
    "\n",
    "# Create subplots in 2 rows, 2 columns\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Top row: Training\n",
    "axs[0, 0].plot(epochs_train, train_loss, label='Train Loss', marker='o', color='red')\n",
    "axs[0, 0].set_title('Training Loss')\n",
    "axs[0, 0].set_xlabel('Step')\n",
    "axs[0, 0].set_ylabel('Loss')\n",
    "axs[0, 0].grid(True)\n",
    "axs[0, 0].set_xticks(np.arange(0, NO_OF_STEPS + 1, 500))\n",
    "axs[0, 0].set_yticks(np.arange(0, max(max(train_loss), max(val_loss)) + 0.1, 0.1))\n",
    "\n",
    "axs[0, 1].plot(epochs_train, train_iou, label='Train IoU', marker='o', color='blue')\n",
    "axs[0, 1].set_title('Training Intersection Over Union (IoU)')\n",
    "axs[0, 1].set_xlabel('Step')\n",
    "axs[0, 1].set_ylabel('IoU')\n",
    "axs[0, 1].grid(True)\n",
    "axs[0, 1].set_xticks(np.arange(0, NO_OF_STEPS + 1, 500))\n",
    "axs[0, 1].set_yticks(np.arange(0, max(max(max(val_iou), max(train_iou)), 1) + 0.01, 0.05))\n",
    "\n",
    "# Bottom row: Validation\n",
    "axs[1, 0].plot(epochs_val, val_loss, label='Validation Loss', marker='o', color='red')\n",
    "axs[1, 0].set_title('Validation Loss')\n",
    "axs[1, 0].set_xlabel('Step')\n",
    "axs[1, 0].set_ylabel('Loss')\n",
    "axs[1, 0].grid(True)\n",
    "axs[1, 0].set_xticks(np.arange(0, NO_OF_STEPS + 1, 500))\n",
    "axs[1, 0].set_yticks(np.arange(0, max(max(train_loss), max(val_loss)) + 0.1, 0.1))\n",
    "\n",
    "axs[1, 1].plot(epochs_val, val_iou, label='Validation IoU', marker='o', color='blue')\n",
    "axs[1, 1].set_title('Validation Intersection Over Union (IoU)')\n",
    "axs[1, 1].set_xlabel('Step')\n",
    "axs[1, 1].set_ylabel('IoU')\n",
    "axs[1, 1].grid(True)\n",
    "axs[1, 1].set_xticks(np.arange(0, NO_OF_STEPS + 1, 500))\n",
    "axs[1, 1].set_yticks(np.arange(0, max(max(max(val_iou), max(train_iou)), 1) + 0.01, 0.05))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8fdc48-c266-4039-a210-f65430d0ef17",
   "metadata": {},
   "source": [
    "## Check model against test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7fe626-a0a2-4bcd-9f19-d90c23ec4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metadata = os.path.join(data_dir, \"test\")\n",
    "test_data = read_jsons(test_metadata)\n",
    "\n",
    "print(f\"Number testing datapoints: {len(test_data)}\")\n",
    "print(f\"Number unique figures for training: {len(set([x['image'] for x in test_data]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce26ca1-1555-411b-b573-26f3b017166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24da371-da2c-45de-b774-84eafab0c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validation = validate(predictor, optimizer, test_data, batch_size=len(test_data))\n",
    "print(f\"Test Set Validation: Batches {curr_validation_output['batches']} | Avg IoU = {test_validation['avg_iou']:.6f} | Avg Loss = {test_validation['avg_loss']:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
